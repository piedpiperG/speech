{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**实验目标：**\n",
    "\n",
    "通过本实验，你将深入了解和实践说话人识别技术，并掌握利用声音特征进行有效说话人识别的基本方法，了解不同特征和模型对识别准确率的影响。\n",
    "\n",
    "实验的核心目标是使用TIMIT数据集来训练一个说话人识别系统，涵盖数据预处理、特征提取、模型训练和评估等关键步骤。\n",
    "\n",
    "\n",
    "**实验方法：**\n",
    "\n",
    "**1. 数据预处理和划分(可选)：**\n",
    "  - 为了方便大家，我们提供了划分好的TIMIT数据集结构，当然你也可以根据训练结果自行划分该原数据集。\n",
    "  - 原数据集下载地址：https://drive.google.com/file/d/180mSIiXN9RVDV2Xn1xcWNkMRm5J5MjN4/view?usp=sharing\n",
    "  - 我们排除了SA的两个方言句子，并在剩余的8个句子中选取了SX的5个句子和SI的1个句子作为训练集，SI的另外2个句子作为测试集。\n",
    "  \n",
    "**2. 特征提取：**\n",
    "  - 学习并实现包括但不限于MFCC特征等特征的提取，探索声音信号的频率和时间特性。\n",
    "  - 鼓励尝试和比较其他特征提取方法，例如LPCC或声谱图特征，以理解不同特征对识别性能的影响。\n",
    "  \n",
    "**3. 模型选择和训练：**\n",
    "  - 探索并选择适合的分类器和模型进行说话人识别，如GMM、Softmax分类器或深度学习模型。\n",
    "  - 实现模型训练流程，使用训练集数据训练模型。\n",
    "  \n",
    "**4. 评估和分析：**\n",
    "  - 使用准确率作为主要的评价指标在测试集上评估模型性能。\n",
    "  - 对比不同特征和模型的性能，分析其对说话人识别准确率的影响。\n",
    "  - 可视化不同模型的识别结果和错误率，讨论可能的改进方法。\n",
    "\n",
    "**实验要求：**\n",
    "  - 1.选择并实现至少一种特征的提取，并鼓励尝试其他特征提取方法。\n",
    "  - 2.选择并实现至少一种分类器或模型进行说话人识别，并使用准确率评估指标评估其性能。\n",
    "  - 3.通过实验对比、分析和可视化，撰写详细的实验报告，包括实验目的、实验方法、结果分析和结论。\n",
    "  - 4.实验报告应以清晰、逻辑性强的形式呈现，图表和结果应清楚明了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 实验准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T07:07:01.106307300Z",
     "start_time": "2024-04-23T07:06:47.853973400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "D:\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "D:\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "D:\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "## 导入必要的库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 可以根据需要导入其他库，比如librosa用于音频处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据预处理(加载数据集)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T07:07:13.793964100Z",
     "start_time": "2024-04-23T07:07:01.086796Z"
    }
   },
   "outputs": [],
   "source": [
    "TrainDir = \"Dataset\\TRAIN\"\n",
    "TestDir = \"Dataset\\TEST\"\n",
    "\n",
    "\n",
    "## 请在这里写代码加载我们划分好的TIMIT训练集和测试集\n",
    "\n",
    "def load_timit_dataset(base_path):\n",
    "    speakers_data = {}\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                speaker_id = root.split(os.sep)[-1]\n",
    "                file_path = os.path.join(root, file)\n",
    "                data, samplerate = sf.read(file_path)\n",
    "                if speaker_id not in speakers_data:\n",
    "                    speakers_data[speaker_id] = []\n",
    "                speakers_data[speaker_id].append((data, samplerate))\n",
    "    return speakers_data\n",
    "\n",
    "\n",
    "train_data = load_timit_dataset(TrainDir)\n",
    "test_data = load_timit_dataset(TestDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T07:09:58.001849200Z",
     "start_time": "2024-04-23T07:09:24.155382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.71472487e+02 -7.13510832e+02 -6.78341201e+02 ... -6.68288882e+02\n",
      "  -6.92413130e+02 -7.32619436e+02]\n",
      " [ 1.68135663e+01  2.32430109e+01  2.65236342e+01 ...  3.81183607e+01\n",
      "   3.26592152e+01  3.47468431e+01]\n",
      " [ 1.95008782e+01 -1.82944429e+00 -2.61476901e+01 ... -1.91550783e+01\n",
      "  -1.14221889e+01  5.88391911e-02]\n",
      " ...\n",
      " [ 7.67943610e+00 -1.07766150e+00 -2.72704264e+00 ... -6.72544537e+00\n",
      "  -2.12274240e+00  1.49642342e+00]\n",
      " [ 5.13097026e+00  1.47870350e+00 -1.76497998e+00 ... -4.47882995e+00\n",
      "  -4.50797310e+00 -5.94221878e+00]\n",
      " [ 4.30375940e+00  5.85914219e+00  1.13321518e+01 ...  1.14590814e+01\n",
      "   7.14151428e+00  6.09138507e+00]]\n"
     ]
    }
   ],
   "source": [
    "## 请编写或使用库函数提取MFCC等音频特征\n",
    "def extract_features(audio_data, samplerate):\n",
    "    mfccs = librosa.feature.mfcc(y=audio_data, sr=samplerate, n_mfcc=13)\n",
    "    return mfccs\n",
    "\n",
    "\n",
    "def prepare_dataset(speakers_data):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for speaker_id, audios in speakers_data.items():\n",
    "        for audio, rate in audios:\n",
    "            mfcc = extract_features(audio, rate)\n",
    "            features.append(mfcc)\n",
    "            labels.append(speaker_id)\n",
    "    return features, np.array(labels)  # 这里将 labels 转换为 numpy 数组\n",
    "\n",
    "\n",
    "train_data_features, train_data_labels = prepare_dataset(train_data)\n",
    "test_data_features, test_data_labels = prepare_dataset(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型选择和训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T14:50:44.573606300Z",
     "start_time": "2024-04-16T14:50:42.428949900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "GaussianMixture(covariance_type='diag', max_iter=200, n_components=462,\n                random_state=0)",
      "text/html": "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianMixture(covariance_type=&#x27;diag&#x27;, max_iter=200, n_components=462,\n                random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianMixture</label><div class=\"sk-toggleable__content\"><pre>GaussianMixture(covariance_type=&#x27;diag&#x27;, max_iter=200, n_components=462,\n                random_state=0)</pre></div></div></div></div></div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 在这部分，你可以选择不同的分类器和模型如GMM模型来进行实验\n",
    "def flatten_features(features):\n",
    "    # 将特征从三维数组转换为二维数组，以便可以用于GMM\n",
    "    flattened_features = []\n",
    "    for feature in features:\n",
    "        # 使用均值跨时间序列来简化数据，可以尝试其他方法如最大值、中位数等\n",
    "        flattened_features.append(np.mean(feature, axis=1))\n",
    "    return np.vstack(flattened_features)\n",
    "\n",
    "\n",
    "# 数据扁平化\n",
    "X_train = flatten_features(train_data_features)\n",
    "X_test = flatten_features(test_data_features)\n",
    "\n",
    "# 标准化特征\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 确定GMM组件的数量\n",
    "n_components = len(np.unique(train_data_labels))\n",
    "\n",
    "# 训练GMM\n",
    "gmm = GaussianMixture(n_components=n_components, covariance_type='diag', max_iter=200, random_state=0)\n",
    "gmm.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 评价指标(准确率Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T14:50:48.026120300Z",
     "start_time": "2024-04-16T14:50:47.908149400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Edgar\\AppData\\Local\\Temp\\ipykernel_16896\\1809277217.py:13: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  labels_mapping[i] = mode(train_data_labels[mask])[0][0]\n",
      "D:\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_stats_py.py:110: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
      "  warnings.warn(\"The input array could not be properly \"\n",
      "C:\\Users\\Edgar\\AppData\\Local\\Temp\\ipykernel_16896\\1809277217.py:13: DeprecationWarning: Support for non-numeric arrays has been deprecated as of SciPy 1.9.0 and will be removed in 1.11.0. `pandas.DataFrame.mode` can be used instead, see https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.mode.html.\n",
      "  labels_mapping[i] = mode(train_data_labels[mask])[0][0]\n"
     ]
    }
   ],
   "source": [
    "## 请编写代码或使用库函数accuracy_score计算测试集上的准确率Accuracy\n",
    "# 对训练数据进行聚类\n",
    "train_labels_gmm = gmm.predict(X_train)\n",
    "\n",
    "# 创建从GMM组件到实际标签的映射\n",
    "from scipy.stats import mode\n",
    "\n",
    "labels_mapping = {}\n",
    "for i in range(n_components):\n",
    "    mask = (train_labels_gmm == i)\n",
    "    # 为每个组件找出最常见的标签\n",
    "    if np.any(mask):\n",
    "        labels_mapping[i] = mode(train_data_labels[mask])[0][0]\n",
    "\n",
    "# 使用映射进行测试数据的预测\n",
    "test_labels_gmm = gmm.predict(X_test)\n",
    "predicted_labels = [labels_mapping[label] for label in test_labels_gmm]\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(test_data_labels, predicted_labels)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  6. 分析和可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T07:57:40.616295300Z",
     "start_time": "2024-04-16T07:57:38.585768700Z"
    }
   },
   "outputs": [],
   "source": [
    "## 请使用matplotlib等可视化库对你的实验结果进行可视化分析。\n",
    "## 包括但不限于准确率的对比、错误分类的分析、特征的影响等。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 结果讨论\n",
    "讨论你的模型性能，尝试解释为什么某些模型比其他模型表现好，以及可能的改进方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 保存模型（可选）\n",
    "如果需要，可以在这里添加代码保存你的模型。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "base",
   "language": "python",
   "display_name": "kernel_base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
